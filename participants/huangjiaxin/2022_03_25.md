# 海量数据处理
> [解决99%的海量数据处理面试题](https://blog.csdn.net/v_july_v/article/details/7382693)
**海量数据处理面试题简介**
- 海量数据处理，是基于海量数据上的存储、处理、操作。何谓海量，就是数据量太大，所以导致要么是无法在较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。
- 处理海量数据问题的通用方法
    - 分而治之/hash映射 + hash统计 + 堆/快速/归并排序；
    - 双层桶划分
    - Bloom filter/Bitmap
    - Trie树/数据库/倒排索引
    - 外排序
    - 分布式处理之Hadoop/Mapreduce

**分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序**
- 适合topk，大数据按频次排序、去重，
- 方法说明
    1. 分而治之/hash映射：针对数据太大，内存受限，只能是：把大文件化成(取模映射)小文件，即16字方针：大而化小，各个击破，缩小规模，逐个解决
    2. hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。
    3. 堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。
- 问题举例
    1. 海量日志数据，提取出某日访问百度次数最多的那个IP。
    2. 寻找热门查询，300万个查询字符串中统计最热门的10个查询
    3. 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。
    4. 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。
    5. 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
    6. 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？
    7. 怎么在海量数据中找出重复次数最多的一个？
    8. 上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。
    9. 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。
    10. 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？
    11. 一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。
    12. 100w个数中找出最大的100个数

**多层划分**
- 方法说明
    - 适用范围：第k大，中位数，不重复或重复的数字
    - 基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行
- 问题举例
    1. 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数
        - 整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。
        - 也可采用分而治之+Hash映射+频次统计，获取频次为1的数
    2. 5亿个int找它们的中位数。
        - 首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。
        - 实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有2^20，就可以直接利用direct addr table进行统计了。






    

